<story-context id=".bmad/bmm/workflows/4-implementation/story-context/template" v="1.0">
  <metadata>
    <epicId>4</epicId>
    <storyId>1</storyId>
    <title>Implement Dynamic Difficulty Adjustment</title>
    <status>drafted</status>
    <generatedAt>2025-12-04</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>docs/sprint-artifacts/4-1-implement-dynamic-difficulty-adjustment.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>game designer</asA>
    <iWant>the AI to dynamically adjust the game's difficulty based on player performance</iWant>
    <soThat>the challenge remains engaging and fair</soThat>
    <tasks>
- [ ] AC 1: Extend `GameSession` to track player performance metrics.
  - [ ] Subtask: Identify relevant player performance metrics (e.g., time taken per puzzle, hints used, number of attempts).
  - [ ] Subtask: Add or update fields in `GameSession.puzzle_state` or `GameSession.game_history` (in `models.py`) to store these metrics.
- [ ] AC 1: Extend `services/game_logic.py` to collect and evaluate performance metrics.
  - [ ] Subtask: Implement functions to collect and process player performance data after puzzle interactions.
- [ ] AC 1: Extend `services/ai_service.py` for dynamic difficulty adjustment.
  - [ ] Subtask: Implement a function to formulate prompts for the Gemini API that include player performance metrics.
  - [ ] Subtask: Guide the AI to subtly adjust puzzle generation parameters (e.g., complexity, number of steps) based on these metrics.
- [ ] AC 1: Create or update Flask API routes for difficulty adjustment.
  - [ ] Subtask: Define a `POST /adjust_difficulty` endpoint in `routes.py` to receive player performance data and trigger AI adjustment.
- [ ] AC 1: Implement unit and integration tests.
  - [ ] Subtask: Write unit tests for the performance metric tracking and evaluation logic in `services/game_logic.py`.
  - [ ] Subtask: Write unit tests for `services/ai_service.py` functions, mocking the Gemini API to verify prompt construction and difficulty adjustment logic.
  - [ ] Subtask: Write integration tests for the `POST /adjust_difficulty` Flask route, verifying API interaction and `GameSession` updates.
  - [ ] Subtask: Manual/Exploratory testing to assess the effectiveness and fairness of dynamic difficulty adjustment.
</tasks>
  </story>

  <acceptanceCriteria>
    <criterion>Given a player's performance in a series of puzzles (e.g., time taken, hints used), when the AI evaluates this performance, then the AI subtly adjusts parameters for future puzzles (e.g., complexity, number of steps) to maintain an optimal challenge level.</criterion>
</acceptanceCriteria>

  <artifacts>
<docs>
    <doc>
        <path>docs/prd.md</path>
        <title>Product Requirements Document: AI Escape</title>
        <section>FR-001: Dynamic Content Generation</section>
        <snippet>The AI dynamically generates and adapts puzzles.</snippet>
    </doc>
    <doc>
        <path>docs/prd.md</path>
        <title>Product Requirements Document: AI Escape</title>
        <section>Future Consideration: Advanced AI</section>
        <snippet>Implementing dynamic difficulty adjustment to maintain player engagement.</snippet>
    </doc>
    <doc>
        <path>docs/architecture.md</path>
        <title>Architecture</title>
        <section>AI Integration (for dynamic difficulty adjustment)</section>
        <snippet>Utilizes Gemini Pro / Gemini 1.5 Pro.</snippet>
    </doc>
    <doc>
        <path>docs/architecture.md</path>
        <title>Architecture</title>
        <section>Prompt Management Strategy</section>
        <snippet>Structured Prompting will incorporate player performance metrics.</snippet>
    </doc>
    <doc>
        <path>docs/architecture.md</path>
        <title>Architecture</title>
        <section>Game State Management & Persistence</section>
        <snippet>`GameSession` (`GameSession.gameHistory` and `GameSession.puzzleState`) will track player performance metrics.</snippet>
    </doc>
    <doc>
        <path>docs/epics.md</path>
        <title>ibe160 - Epic Breakdown</title>
        <section>Epic 4: Expanding Variety and Replayability</section>
        <snippet>This Epic aims to significantly increase the breadth of content and player choice within the "AI Escape" game, delivering on the promise of endless replayability.</snippet>
    </doc>
    <doc>
        <path>docs/epics.md</path>
        <title>ibe160 - Epic Breakdown</title>
        <section>Story 4.1: Implement Dynamic Difficulty Adjustment</section>
        <snippet>Story 4.1 focuses on enabling the AI to dynamically adjust game difficulty based on player performance, ensuring an engaging and fair challenge.</snippet>
    </doc>
    <doc>
        <path>docs/ux-design-specification.md</path>
        <title>AI Escape - UX Design Specification</title>
        <section>5. User Journey Flows</section>
        <snippet>UX Design outlines user journey flows for New Game Creation and Load Game, which will rely on the backend's game state management to function correctly. This implies the need for dynamic content in the UI.</snippet>
    </doc>
</docs>
    <code>
        <code-ref>
            <path>models.py</path>
            <kind>file</kind>
            <reason>Extend `GameSession` to track player performance metrics</reason>
        </code-ref>
        <code-ref>
            <path>services/game_logic.py</path>
            <kind>file</kind>
            <reason>Extend to collect and evaluate performance metrics</reason>
        </code-ref>
        <code-ref>
            <path>services/ai_service.py</path>
            <kind>file</kind>
            <reason>Extend for dynamic difficulty adjustment (formulate prompts with player performance metrics)</reason>
        </code-ref>
        <code-ref>
            <path>routes.py</path>
            <kind>file</kind>
            <reason>Create or update Flask API routes (`POST /adjust_difficulty`)</reason>
        </code-ref>
        <code-ref>
            <path>tests/</path>
            <kind>directory</kind>
            <reason>For new unit and integration tests</reason>
        </code-ref>
    </code>
    <dependencies>
      <python>
        <package name="Flask" version="3.1.2" />
        <package name="python-dotenv" version="?" />
        <package name="gunicorn" version="?" />
        <package name="pytest" version="?" />
        <package name="google-generativeai" version="0.8.5" />
        <note>Core Python dependencies for the Flask application and Google Gemini API client.</note>
      </python>
      <node>
        <package name="playwright" version="?" />
        <note>Existing Node.js project dependencies for E2E testing.</note>
      </node>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint>
        <type>Required Pattern</type>
        <description>Python Flask application structure (modular, with `app.py`, `static/`, `templates/`, `models.py`, `routes.py`, `services/`, `tests/`).</description>
        <source>docs/architecture.md#Project-Structure</source>
    </constraint>
    <constraint>
        <type>Required Pattern</type>
        <description>WCAG 2.1 Level AA accessibility compliance.</description>
        <source>docs/architecture.md#Accessibility-Implementation</source>
    </constraint>
    <constraint>
        <type>Required Pattern</type>
        <description>Stateless JWT Authentication.</description>
        <source>docs/architecture.md#Security-Architecture</source>
    </constraint>
    <constraint>
        <type>Layer Restriction</type>
        <description>Frontend (Jinja2 templates/Static assets) &lt;-> Backend (Flask Routes/API Endpoints).</description>
        <source>docs/architecture.md#Integration-Points</source>
    </constraint>
    <constraint>
        <type>Layer Restriction</type>
        <description>Flask Routes/API Endpoints &lt;-> AI Service (Gemini API).</description>
        <source>docs/architecture.md#Integration-Points</source>
    </constraint>
    <constraint>
        <type>Layer Restriction</type>
        <description>Flask Routes/API Endpoints &lt;-> Database (Supabase PostgreSQL via SQLAlchemy).</description>
        <source>docs/architecture.md#Integration-Points</source>
    </constraint>
    <constraint>
        <type>Testing Requirement</type>
        <description>Unit Tests (Pytest with `pytest-mock`).</description>
        <source>docs/architecture.md#Comprehensive-Testing-Strategy</source>
    </constraint>
    <constraint>
        <type>Testing Requirement</type>
        <description>Integration Tests (Pytest).</description>
        <source>docs/architecture.md#Comprehensive-Testing-Strategy</source>
    </constraint>
    <constraint>
        <type>Testing Requirement</type>
        <description>E2E Tests (Playwright).</description>
        <source>docs/architecture.md#Comprehensive-Testing-Strategy</source>
    </constraint>
    <constraint>
        <type>Coding Standard</type>
        <description>Python Classes: `PascalCase`, Python Functions/Variables: `snake_case`, Python Modules/Files: `snake_case`, Database Tables: `snake_case` (plural), Database Columns: `snake_case`, API Endpoints: `kebab-case` for URL paths, `snake_case` for query parameters, Constants: `SCREAMING_SNAKE_CASE`.</description>
        <source>docs/architecture.md#Consistency-Rules</source>
    </constraint>
    <constraint>
        <type>Coding Standard</type>
        <description>Code Organization: Modular Flask application layout.</description>
        <source>docs/architecture.md#Code-Organization</source>
    </constraint>
</constraints>

<interfaces>
    <interface>
        <name>/adjust_difficulty</name>
        <kind>REST endpoint</kind>
        <signature>POST /adjust_difficulty</signature>
        <path>routes.py</path>
    </interface>
</interfaces>
  <tests>
    <standards>
      <standard>
        <framework>Pytest</framework>
        <type>Unit, Integration</type>
        <note>Unit tests should mock the Gemini API.</note>
        <source>docs/architecture.md#Testing-Strategy</source>
      </standard>
      <standard>
        <framework>Playwright</framework>
        <type>E2E</type>
        <source>docs/architecture.md#Comprehensive-Testing-Strategy</source>
      </standard>
    </standards>
    <locations>
      <location>tests/</location>
      <note>Mirroring the application's module structure (e.g., `tests/unit/`, `tests/integration/`).</note>
    </locations>
    <ideas>
      <idea ac="1">
        <description>Write unit tests for the performance metric tracking and evaluation logic in `services/game_logic.py`.</description>
      </idea>
      <idea ac="1">
        <description>Write unit tests for `services/ai_service.py` functions, mocking the Gemini API to verify prompt construction and difficulty adjustment logic.</description>
      </idea>
      <idea ac="1">
        <description>Write integration tests for the `POST /adjust_difficulty` Flask route, verifying API interaction and `GameSession` updates.</description>
      </idea>
      <idea ac="1">
        <description>Manual/Exploratory testing to assess the effectiveness and fairness of dynamic difficulty adjustment.</description>
      </idea>
    </ideas>
  </tests>
</story-context>